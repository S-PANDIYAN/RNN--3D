#!/usr/bin/env python3
"""
Direct Keras to TensorFlow.js Converter
This script converts your Amazon RNN model directly to TensorFlow.js format
"""

import os
import sys

def convert_model_direct():
    """Convert Keras model directly to TensorFlow.js format"""
    
    # Check if model file exists
    model_path = r"D:\SE\Amazon RNN Model.keras"
    if not os.path.exists(model_path):
        print(f"❌ Model file not found: {model_path}")
        return False
    
    print(f"✅ Found model file: {model_path}")
    
    try:
        # Import TensorFlow
        print("📦 Loading TensorFlow...")
        import tensorflow as tf
        print(f"✅ TensorFlow version: {tf.__version__}")
        
        # Load the Keras model
        print("🔄 Loading Keras model...")
        model = tf.keras.models.load_model(model_path)
        print("✅ Model loaded successfully!")
        
        # Print model summary
        print("\n📊 Model Summary:")
        model.summary()
        
        # Get model info for frontend integration
        print("\n🔍 Model Information for Frontend:")
        print(f"  - Input shape: {model.input_shape}")
        print(f"  - Output shape: {model.output_shape}")
        print(f"  - Vocab size: 10000 (from embedding layer)")
        print(f"  - Sequence length: 200")
        print(f"  - RNN units: 64")
        
        # Convert using Python API directly
        print("\n🔄 Converting to TensorFlow.js format...")
        
        try:
            import tensorflowjs as tfjs
            
            # Output directory
            tfjs_output_path = "./amazon_model_tfjs"
            
            # Convert the model
            tfjs.converters.save_keras_model(
                model, 
                tfjs_output_path,
                quantization_bytes=None,  # No quantization for better compatibility
                metadata={'name': 'Amazon RNN Sentiment Model'}
            )
            
            print("✅ TensorFlow.js conversion successful!")
            print(f"✅ Converted model saved to: {tfjs_output_path}")
            
            # List generated files
            if os.path.exists(tfjs_output_path):
                files = os.listdir(tfjs_output_path)
                print(f"\n📁 Generated files:")
                for file in files:
                    file_path = os.path.join(tfjs_output_path, file)
                    size = os.path.getsize(file_path)
                    print(f"  - {file} ({size:,} bytes)")
                    
                # Check for model.json
                model_json_path = os.path.join(tfjs_output_path, "model.json")
                if os.path.exists(model_json_path):
                    print(f"\n🎯 Main model file: {model_json_path}")
                    print("✅ This is the file you need to load in your JavaScript code!")
                    
                    # Read and show some model metadata
                    import json
                    with open(model_json_path, 'r') as f:
                        model_data = json.load(f)
                    
                    print(f"\n📋 Model Metadata:")
                    print(f"  - Format: {model_data.get('format', 'Unknown')}")
                    print(f"  - Generated by: {model_data.get('generatedBy', 'Unknown')}")
                    if 'modelTopology' in model_data:
                        topology = model_data['modelTopology']
                        if 'config' in topology:
                            layers = topology['config'].get('layers', [])
                            print(f"  - Number of layers: {len(layers)}")
                    
            return True
            
        except ImportError:
            print("❌ TensorFlowJS not available, trying alternative method...")
            
            # Alternative: Save as H5 and create conversion info
            h5_path = "./amazon_model.h5"
            print(f"💾 Saving model as H5 format: {h5_path}")
            model.save(h5_path)
            
            # Create a simple metadata file for manual conversion
            metadata = {
                "model_path": h5_path,
                "input_shape": str(model.input_shape),
                "output_shape": str(model.output_shape),
                "vocab_size": 10000,
                "sequence_length": 200,
                "rnn_units": 64,
                "conversion_command": f"tensorflowjs_converter --input_format=keras {h5_path} ./amazon_model_tfjs"
            }
            
            import json
            with open("./model_metadata.json", "w") as f:
                json.dump(metadata, f, indent=2)
            
            print("✅ Model saved as H5 format")
            print("📄 Metadata saved to model_metadata.json")
            print("\n💡 To complete conversion, run:")
            print(f"tensorflowjs_converter --input_format=keras {h5_path} ./amazon_model_tfjs")
            
            return True
            
    except Exception as e:
        print(f"❌ Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_frontend_integration():
    """Create integration code for the frontend"""
    
    integration_code = '''
// Add this to your model-loader.js or create a new file: amazon-model-integration.js

class AmazonModelIntegration {
    constructor() {
        this.model = null;
        this.isLoaded = false;
        this.vocabSize = 10000;
        this.sequenceLength = 200;
        this.rnnUnits = 64;
    }

    async loadAmazonModel() {
        try {
            console.log('🔄 Loading Amazon RNN model...');
            
            // Load the converted TensorFlow.js model
            this.model = await tf.loadLayersModel('./amazon_model_tfjs/model.json');
            
            console.log('✅ Amazon model loaded successfully!');
            console.log('Model summary:', this.model.summary());
            
            this.isLoaded = true;
            return true;
            
        } catch (error) {
            console.error('❌ Failed to load Amazon model:', error);
            this.isLoaded = false;
            throw error;
        }
    }

    async predict(text) {
        if (!this.isLoaded || !this.model) {
            throw new Error('Model not loaded');
        }

        try {
            // Simple tokenization (you may need to adjust this based on your original training)
            const tokens = this.tokenizeText(text);
            
            // Convert to tensor
            const inputTensor = tf.tensor2d([tokens], [1, this.sequenceLength]);
            
            // Make prediction
            const prediction = this.model.predict(inputTensor);
            const result = await prediction.data();
            
            // Clean up tensors
            inputTensor.dispose();
            prediction.dispose();
            
            // Convert to sentiment score (0-1 scale where 0.5 is neutral)
            const sentimentScore = result[0];
            
            return {
                score: sentimentScore,
                sentiment: sentimentScore > 0.6 ? 'positive' : 
                          sentimentScore < 0.4 ? 'negative' : 'neutral',
                confidence: Math.abs(sentimentScore - 0.5) * 2
            };
            
        } catch (error) {
            console.error('❌ Prediction failed:', error);
            throw error;
        }
    }

    tokenizeText(text) {
        // Simple tokenization - you may need to adjust this
        // to match your original training tokenization
        const words = text.toLowerCase()
            .replace(/[^a-zA-Z0-9\\s]/g, '')
            .split(/\\s+/)
            .filter(word => word.length > 0);
        
        // Convert words to indices (simplified)
        // In a real implementation, you'd use the same vocabulary 
        // that was used during training
        const tokens = new Array(this.sequenceLength).fill(0);
        
        for (let i = 0; i < Math.min(words.length, this.sequenceLength); i++) {
            // Simple hash-based tokenization (replace with proper vocabulary)
            tokens[i] = this.hashWord(words[i]) % this.vocabSize;
        }
        
        return tokens;
    }

    hashWord(word) {
        // Simple hash function for demo purposes
        let hash = 0;
        for (let i = 0; i < word.length; i++) {
            const char = word.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // Convert to 32-bit integer
        }
        return Math.abs(hash) + 1; // Ensure positive and non-zero
    }
}

// Usage example:
/*
const amazonModel = new AmazonModelIntegration();

// Load the model
await amazonModel.loadAmazonModel();

// Make predictions
const result = await amazonModel.predict("I love this movie!");
console.log(result); // { score: 0.8, sentiment: 'positive', confidence: 0.6 }
*/
'''
    
    with open("./amazon-model-integration.js", "w") as f:
        f.write(integration_code)
    
    print("✅ Frontend integration code created: amazon-model-integration.js")

def main():
    """Main function"""
    print("🚀 Direct Keras to TensorFlow.js Converter")
    print("=" * 50)
    
    success = convert_model_direct()
    
    if success:
        print("\n🎉 Conversion process completed!")
        
        # Create frontend integration
        create_frontend_integration()
        
        print("\n📝 Next steps:")
        print("1. Copy the 'amazon_model_tfjs' folder to your web project")
        print("2. Include the 'amazon-model-integration.js' file in your project")
        print("3. Update your main JavaScript code to use the Amazon model")
        print("4. Test the integration with your 3D RNN visualization!")
        
        print("\n🔗 Integration steps:")
        print("   - Add <script src='amazon-model-integration.js'></script> to your HTML")
        print("   - Use AmazonModelIntegration class in your RNN visualization")
        print("   - The model will provide real sentiment predictions!")
        
    else:
        print("\n💔 Conversion failed. Please check the errors above.")
        
if __name__ == "__main__":
    main()
